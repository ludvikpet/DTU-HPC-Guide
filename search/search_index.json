{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to The Hitchhiker's Guide to the HPC!","text":"<p>This tutorial is an independent, beginner-oriented introduction to the DTU High-Performance Computing (HPC) system. It is intended as a practical supplement to the official DTU HPC documentation, not a replacement. For users experienced with linux CLI, we advise you to start here instead.</p> <p>We aditionally provide a GitHub repository which aids in establishing a viable project structure as well as an interactive code debugger in order for you to get started with your project as quickly as possible.</p>"},{"location":"#attribution-and-citation","title":"Attribution and Citation","text":"<p>This tutorial was developed by Ludv\u00edk Petersen and Martin \u00c6gidius for the Section of Visual Computing, Technical University of Denmark (DTU).</p> <p>The material is a practical extension of the official DTU HPC documentation provided by DTU Computing Center (DCC). Users are encouraged to consult the official resources for authoritative and up-to-date information:</p> <ul> <li>https://www.hpc.dtu.dk/</li> </ul> <p>If you use this tutorial as part of your work, please cite the DTU Computing Center (DCC) as the primary source of HPC system documentation:</p> <ul> <li>https://www.hpc.dtu.dk/?page_id=4061</li> </ul> <p>If you directly reuse or adapt code from the accompanying GitHub repository in an unmodified form, please also cite this tutorial as:</p> <p>Petersen, L., &amp; \u00c6gidius, M. The Hitchhiker\u2019s Guide to the DTU HPC. Technical University of Denmark, 2026.</p>"},{"location":"10_jupyter/","title":"Jupyter notebooks on the cluster","text":"<p>While, in general, it is ill advised to use Jupyter notebooks in most ML/DL tasks, they may find specific valid use cases such as for plotting or rapid prototyping. You can run jupyter notebooks directly on the HPC from interactive nodes through VSCode. To do so, use the remote ssh extension to login to a login node, open the terminal and start an interactive session. Now install the Jupyter extension in VSCode:</p> <p></p> <p>In addition, load your virtual environment and install the following packages: </p> <pre><code>python -m pip install notebook ipykernel ipython\n</code></pre> <p>Now you can open/create a jupyter notebook (.ipynb) and run it on the HPC by choosing \"Select kernel\" in the upper right, and choosing your virtual environment. Executing cells, etc., is the same as on your local machine. </p>"},{"location":"11_monitoring/","title":"Monitoring machine learning projects","text":"<p>When training Deep Learning models, it is necessary to keep track of their behaviour. Which model parameters did I choose? Did the model converge correctly? Does the output of the model seem to generalize well to the underlying data? </p> <p>For a single model, this is not so difficult to monitor. Generally, however, you'd like to tweak your model in order to optimize performance. Tracking the nuances of each run can become very difficult in practice, which makes it difficult to manage e.g. the reproducibility of a previously successful experiment.</p> <p>For this, your friend and ally is Weights &amp; Biases, also known as wandb. This framework is intended to streamline monitoring by broadcasting all your experiments onto your personal page on wandb.ai . To use the framework, you'll need to register to wandb - navigate to the wandb home page and sign up.</p> <p>Broadly, wandb allows you to create sub-folders for your experiments (neat if only a subset is coupled), upload plots and histograms, create sub-sections for your train-val-test splits, upload images, files and much more. Many of these features can also be made temporal, i.e. for a given model, its output at epochs 10 and 100 can easily and intuitively be compared to each other. More advanced tools are also available such as model sweeps, which automate hyperparameter search whilst producing rich visualizations, simplifying parameter tuning significantly in some instances.</p> <p>As Weights &amp; Biases themselves have a perfectly fine guide, we direct you to the following site for further guidance: Wandb guide.</p>"},{"location":"12_best_practices/","title":"Best-practices for a successful project","text":"<ol> <li>Always use strong passwords, trusted data, trusted GitHub repositories, trusted networks and trusted machines when communicating with the HPC system. Never share any private ssh keys or ip adresses. If the HPC suddenly is down due to malware/attacks, it not only hurts your project, but also everyone elses. Remember that security is your responsibility. </li> <li>Only use Jupyter Notebooks when it really makes your process faster. They are a common source of bugs. </li> <li>Make your scripts with many print-statements of e.g. tensor shapes. This sometimes allows you to know the source of errors without having to re-do the run. Even better, use a logger (we recommend <code>loguru</code>). The output will be in your output-file when running on compute nodes. You can always print the last n lines using the command: <code>tail -n &lt;n&gt; myfile.out</code></li> <li>Make naming conventions for files and functions - and stick to them</li> <li>Modularize functions and code such that it may be re-used as much as possible.</li> <li>Always run code from the project-root - that is, consistently invoke python scripts using <code>python3 src/your_python_file.py</code> and similar. This ensures that all path handling is constant across all scripts. Many (expensive) bugs occur due to path handling - e.g. saving results to a path which doesn't exist after finishing expensive operations; then you would have to re-run the whole pipeline.</li> <li>For deep learning and machine learning algorithms using iterative optimization: save model checkpoints and optimizer state many times throughout training. If a bug occurs, you can continue from the last checkpoint and save valuable time</li> <li>Always seed your algorithms. This ensures reproducibility and aids in interpretation of parameter-influence.</li> <li>Use professional experimental monitoring, such as Weights &amp; Biases. This can save a lot of time and headache. </li> <li>Tune your batch-scripts such that you only request the amount of resources you actually need. In busy periods, this may allow you to gain a slot quickly on the compute queue.</li> <li>Debug your batch-scripts on an interactive node before submitting them to a queue such that you in advance know it will not fail due to shell-script bugs. </li> </ol>"},{"location":"1_access_cluster/","title":"Acessing the cluster","text":"<p>The cluster is accessed using ssh. ssh is an abbreviation for secure shell, a network protocol which allows two computers to communicate through a network tunnel. In order to be able to access the cluster using ssh, your computer needs to authenticate itself to the HPC, such that the HPC machine knows that you are, indeed, you. </p> <p>This authentication is either achieved by being logged onto a DTU WIFI network, the DTU VPN or by using a private and public ssh key pair. </p>"},{"location":"1_access_cluster/#prerequisites","title":"Prerequisites","text":"<p>You may only connect to DTU systems via machines that you trust, i.e. you have no malware infections. This also means that your OS and other software still has to receive regular updates and are updated to newest versions, as they else may not be secure. </p>"},{"location":"1_access_cluster/#using-the-dtu-vpn","title":"Using the DTU VPN","text":"<p>If you only need to use the cluster for a couple of days, using the DTU VPN is fine, and it simplifies setup significantly. </p> <p>In this case, start and connect to the VPN service, and subsequently open a terminal and write: </p> <pre><code>ssh s123456@login1.gbar.dtu.dk\n</code></pre> <p>press enter <code>\u21b5</code>, and enter the DTU password you always use to login into DTU services. Now move to the other sections of the tutorial. </p> Windows users <p>You will need to use a terminal which supports ssh. We recommend Windows PowerShell but you can also use Windows Terminal. In this guide, all commands are provided as Linux commands, and most work the same in PowerShell. </p> <p>Note: you will need to keep the VPN connection open to stay connected. </p>"},{"location":"1_access_cluster/#using-a-ssh-keypair","title":"Using a ssh keypair","text":"<p>If you're doing a longer project, setting up the ssh key pair is the best approach. To do so, open a terminal and make a directory called <code>.ssh</code> in your home root by entering the following in the terminal: </p> <pre><code>mkdir -p ~/.ssh\n</code></pre> <p>Here, <code>-p</code> is a flag to the command which allows parent folder creation, and does not give an error if the folder already exists. Now change the working directory to the newly created directory issuing: </p> <pre><code>cd .ssh \n</code></pre> <p>and pressing enter <code>\u21b5</code>. <code>cd</code> stands for \"change directory\". Now it's time to generate the public and private ssh key pair. Enter the following command in your terminal: </p> <pre><code>ssh-keygen -t ed25519 -f gbar\n</code></pre> <p>and press enter <code>\u21b5</code>.  Now, you must enter a passphrase. This passphrase needs to be secure and secret, and cannot be the same passphrase which you use for your DTU login. Note that you cannot directly see in the terminal how many characters you have written, so take care. </p> <p>This will create two files in your .ssh-directory: <code>gbar</code> and <code>gbar.pub</code>. Check this by entering </p> <pre><code>ls -la\n</code></pre> <p>in your terminal. This lists all files and folders in the current working directory; <code>ls</code> is short for list. Check that the two files are present in the output. </p> <p>Important: The <code>gbar</code> file is your private file and may in no way be shared to any online service, as this would could allow adversaries to connect to the HPC services using your identity. </p> <p>The next step is to copy the content of the public key <code>gbar.pub</code> to the HPC server.  Then, the HPC will accept an incoming connection from your PC. </p> <p>To do so, connect to the DTU VPN or connect to an on-premise DTU Wi-Fi network. Then ssh into the HPC service using a terminal and entering:</p> <pre><code>ssh s123456@login1.gbar.dtu.dk\n</code></pre> <p>and authorizing with your normal DTU password. It will ask you whether you want to accept the connection for a specific fingerprint. Check that the fingerprint matches the one provided here and else decline as someone may be trying to attack your connection, and the network connection is not trustworthy! If they match, continue.  </p> <p>Once on the server, make a .ssh folder using the following command:</p> <pre><code>mkdir -m 700 -p ~/.ssh\n</code></pre> <p>here, <code>-m</code> is a flag which sets the mode of the folder as being only accessible by the owner (you) and the root (HPC admin). External adversaries will therefore not be able to access the folder contents. </p> <p>cd into the directory, then write:</p> <pre><code>cat &gt;&gt; authorized_keys\n</code></pre> <p>and paste the complete contents of your gbar.pub file into the terminal and press enter <code>\u21b5</code>. This appends a new line to the file authorized_keys. Then close the file by pressing enter <code>\u21b5</code> followed by <code>ctrl</code> + <code>d</code>.</p> <p>This file tells the system to accept incoming connections from your user on your machine. Finally, the permissions of the file need to be set to \"600\", which can be modified using the following command: </p> <pre><code>chmod 600 authorized_keys\n</code></pre> <p>which is an abbreviation for change mode. You will also need to use this command for making scripts executable, etc.</p> <p>Now, you should be able to connect without the VPN active and from any network.  Please remember still to be cautious and only login from networks which you know you can trust.  </p> <p>If you want to simplify the login procedure further, you can create a file in your local <code>.ssh</code> directory called <code>config</code>  and insert the following information in it: </p> <pre><code>Host gbar\nUser s123456\nIdentityFile ~/.ssh/gbar\nHostname login1.gbar.dtu.dk\n</code></pre> <p>Note, that you need to change the User line to fit with your own student number or DTU username. Using this, you can simply run <code>ssh gbar</code> in your terminal to connect.</p>"},{"location":"1_access_cluster/#new-commands-in-this-section","title":"New commands in this section","text":"<p>In this section the following commands have been used:</p> Command Function <code>mkdir &lt;new_folder_name&gt;</code> create a new directory <code>cd &lt;sub-directory&gt;</code> Change working directory. Go back one level with <code>cd ..</code> <code>ls -la</code> List contents of a directory. <code>-la</code> shows all files, including hidden ones <code>chmod &lt;permission-code&gt; &lt;your_file&gt;</code> Change file permissions <code>cat &gt;&gt; &lt;some_file&gt;</code> Append a line to <code>some_file</code>. Creates the file if it does not exist <code>cat &lt;some_file&gt;</code> Print file contents <code>ssh &lt;user-id&gt;@&lt;host-server&gt;</code> Establish a secure shell connection from local machine to host server"},{"location":"2_vscode_setup/","title":"VSCode for remote development","text":"<p>We strongly recommend using VSCode to program/interact with the HPC for beginners. While it is not strictly necessary, it can speed up the process of coding significantly, and feel almost just like coding locally on your own computer. </p>"},{"location":"2_vscode_setup/#setting-up-the-remote-ssh-extension","title":"Setting up the Remote - SSH extension","text":"<p>After you've installed and opened VSCode, navigate to the extensions pane and install the Remote - SSH extension by Microsoft: </p> <p>After it is installed, press the two arrows in the bottom left corner of the editor. Choose \"Connect to host\" and choose gbar from the drop-down or manually add the host login1.gbar.dtu.dk. After entering your passphrase, a new editor will open which is a VSCode session on the cluster login node. The attached terminal works just like the terminal you would have after ssh'ing into the HPC system locally (e.g. using Windows PowerShell).</p> <p>Notice how the left pane of the editor is a file explorer. Try clicking \"Open folder\" and pressing enter. This will make your current path the home folder of your HPC user. You may create, delete, move and copy files and folders using the file editor. You can also drag and drop local files from your computer to the file editor pane to copy files to the cluster. Note: you may only do this for smaller code files and folders, nothing large, as this will clog up the login node.</p> <p>Now, navigate to the extensions panel and install the <code>Python</code> and <code>Python Debugger</code> extensions by Microsoft. Although you might have them installed locally, you will also need to have them installed in the HPC VSCode editor. </p> <p>If you are coding your project by yourself and you don't want to bother using git, then you may simply create a project folder, make an environment, and begin coding. If you are collaborating with someone, we advise you to set up GitHub. </p>"},{"location":"3_github_setup/","title":"Setting up GitHub for collaboration","text":"<p>In order to collaborate with peers, you're probably interested in setting up a working GitHub environment on the cluster. This allows you to push code to and from the cluster into an online repository such that you can </p> <ol> <li>communicate efficiently between the cluster and your local computer</li> <li>collaborate efficiently with peers.</li> </ol> <p>This process is equivalent to what you would do on your local machine. If you're unfamiliar with the process, this section acts a a GitHub setup walkthrough to get you up and running.</p>"},{"location":"3_github_setup/#creating-and-using-a-ssh-keypair-for-github-authentication","title":"Creating and using a ssh keypair for GitHub authentication","text":"<p>We will again use an ssh-keypair to allow the connection between your Github account and your user on the HPC system, following this guide. </p> <ol> <li>Create and login to an account on github.com. </li> <li><code>ssh</code> into the HPC system from a terminal or using VSCode.</li> <li>enter <code>ssh-keygen -t ed25519 -C \"your_email@example.com\"</code> in the terminal. Here, the email must the same as used in GitHub.</li> <li>Enter a passphrase which you can remember (which again should be different than the one used for your GitHub account)</li> <li>Enter <code>ssh-add ~/.ssh/id_ed25519</code> into the terminal to add it to the ssh-agent</li> <li>Copy the contents of the file ~/.ssh/id_ed25519.pub, e.g. using <code>cat ~/.ssh/id_ed25519.pub</code> and copying the contents to your clipboard </li> <li>Open GitHub in a browser.</li> <li>Click on your profile picture</li> <li>Click on Settings</li> <li>Click on SSH and GPG keys</li> <li>Press New SSH key</li> <li>Give the key the name \"HPC\", and paste the public key content from your clipboard into key field.</li> <li>Press Add SSH key</li> </ol> <p>Remember, to never share your private ssh keys to GitHub or anywhere else.  The process is the exact same on your own machine. If you plan on editing code locally and pushing to the HPC, it may be beneficial to setup a ssh key pair for your local machine as well.</p>"},{"location":"3_github_setup/#creating-a-new-repository","title":"Creating a new repository","text":"<p>Creating a repository is simple, and may be done using following these steps: </p> <ol> <li>Create a repository in your browser on GitHub.com.</li> <li>After creating it, click on the green &lt; &gt; Code button and click on SSH. Copy the URL to your clipboard.\u0344</li> </ol> <p>Windows/Mac/some Linux users</p> <p>You will need to install git on your system before executing the next steps.</p> <ol> <li><code>ssh</code> into the HPC. Using the HPC terminal, write <code>git clone &lt;your_pasted_url&gt;</code> and press enter.</li> <li>Authenticate with your GitHub ssh passphrase.</li> <li>Now your GitHub repository is downloaded into a project folder. <code>cd</code> into it.</li> <li>Repeat steps 1-5 in a terminal on your own machine</li> <li>Steps 7-11 should be carried out on the HPC terminal. Make a new textfile by writing <code>echo \"test\" &gt;&gt; helloworld.txt</code>. </li> <li>Type <code>git add helloworld.txt</code></li> <li>Type <code>git commit -m \"First comission to my new repository!\"</code></li> <li>Type <code>git push origin main</code></li> <li>Authenticate with your ssh passphrase</li> <li>Open the repository on GitHub.com. You should be able to see that the repository contains the new file</li> <li>On your local machine, in a terminal in the project folder, type \"git pull origin main\" and authenticate. This will pull new pushes to the GitHub repository down onto your local machine</li> </ol> <p>While this process is nice for communicating with the HPC in an efficient manner, the real strength lies in the fact that you can invite other users to your GitHub repository on the GitHub webpage. Everyone is then able to edit, push and pull code. </p>"},{"location":"3_github_setup/#using-the-template-project-and-pushing-to-your-own-repository","title":"Using the template project and pushing to your own repository","text":"<p>This can easily be done by following these steps: </p> <ol> <li>Make a completely empty GitHub repository and copy the URL as in step 2 when creating a new repository</li> <li><code>git clone &lt;template_project_URL&gt;</code></li> <li>Rename the folder by writing <code>mv HPCProjectTemplate &lt;your_project_name&gt;</code></li> <li><code>cd</code> into <code>&lt;your_project_name&gt;</code></li> <li>write <code>git remote set-url origin &lt;your_git_repo_URL&gt;</code>. This tells git that the project should be linked to this repository instead</li> <li>write <code>git push -u origin main</code>. This pushes all the local changes into your repository, and you may from now on use it as such. </li> </ol>"},{"location":"3_github_setup/#git-command-cheat-sheet","title":"Git command cheat-sheet","text":"<p>At its core, managing git is quite simple, only needing a small set of crucial commands that you should remember. These are:</p> Command Function <code>git add &lt;yourfile1&gt; &lt;yourfile2&gt; ...</code> Tell git that you have done changes to these files and want to push them to the repository. If instead of filenames you just write \".\", it will take all files which you have changed. <code>git commit -m \"&lt;your_message_here&gt;\"</code> Bundle your changes with a commit message. Your commit message should describe why you are pushing, what you fixed, etc. <code>git push origin &lt;branch_name&gt;</code> push your submission to the specified branch of your repository. The default branch is called main. <code>git pull</code> pull updates from the main branch of the repository <p>Here, it is important to understand that <code>git add</code> takes a snapshot of the files as they are now. If you change the file after running <code>git add</code>, you will have to re-add it. In principle, usually you would change some files. Then you would add them in smaller comissions at a time such that each commit contains logically codependent content which may be well described with a message, and make a push for each bundle.</p> <p>Many more Git commands exist, which you may find a use for. If you'd like an overview of these, we refer you to the official GitHub command Cheat-sheet.</p>"},{"location":"3_github_setup/#merge-conflicts","title":"Merge conflicts","text":"<p>Imagine that user A and B both pull from a repository, and both change the same line of the same file shared_file.txt. Now both users try to push their changes, but user A pushed before user B. The repository will then in most cases not accept the push by user B, as the push conflicts with the current contents of shared_file.txt. User B will then have to pull the changes first, and then will have to merge their local changes into shared_file.txt in a specific way such that git knows which of the changes you then want to keep. This process is called merging. </p> <p>Getting good at git is hard work, and is covered in other DTU courses, and many excellent online resources exist, e.g. git - the simple guide. If you are doing a larger code-heavy project with one or multiple collaborators, it is strongly advised that you learn git as it will allow you to work more efficiently in parallel. </p>"},{"location":"4_project_setup/","title":"Setting up a project on the HPC","text":"<p>Having gained access to the cluster and setup GitHub, we would now like to create a project folder. This can be done as you'd usually do on Visual Studio Code using the editor's file explorer, or by using the <code>mkdir</code> command in the terminal.</p>"},{"location":"4_project_setup/#project-structure","title":"Project structure","text":"<p>The structure of the given folder is not pivotal, however, we highly recommend using the structure provided in our GitHub repository. This will save you a lot of trouble later on when e.g. debugging. We also include some relevant files, which will come in handy later on. You can simply clone this repository or copy the contents into your project folder. The outline of our repository is found below:</p> <pre><code>\u2514\u2500\u2500 data\n    \u251c\u2500\u2500 data_file1.npy\n    \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 logs\n    \u251c\u2500\u2500 log-1.txt\n    \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 outputs\n    \u251c\u2500\u2500 out.txt\n    \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 scripts\n    \u251c\u2500\u2500 add_debug_machine.sh\n    \u251c\u2500\u2500 run_file.sh\n    \u251c\u2500\u2500 setup_env.sh\n    \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 main.py\n    \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 README.md\n</code></pre> <p>Here, the src directory is intended for your <code>python</code> files, scripts holds all shell script files, logs holds all your logs for your batch jobs, outputs holds all logs when running in an interactive node session and data holds all your data files.</p> <p>When invoking any of the scripts, always call them from the project root folder, i.e. run main.py by writing <code>python3 src/main.py</code> - this consistency will save you a lot of time in the long run, as wrong path structure is a huge source of bugs. </p>"},{"location":"4_project_setup/#opening-the-project-in-vscode","title":"Opening the project in VSCode","text":"<p>Having created our project directory, we'd now like to step into it with our VS Code editor. This will make life a lot easier in the future. To do so, we first step into the folder using the terminal. Open a terminal and <code>cd</code> into your project directory. Once at the project root, we run the command:</p> <pre><code>pwd\n</code></pre> <p><code>pwd</code>, or print working directory, does as the name suggests. Having run the command, now copy the console output.</p> <p>Now in your editor, navigate to File \u2192 Open Folder, which will display a file explorer at the top of your screen. Paste the path to your project directory here and click OK; your editor is now located at your project root!</p> <p>For future reference, this navigation is made much easier each time you connect to the server - just append the extra path elements onto the path string that you're prompted with initially.</p>"},{"location":"5_python_setup/","title":"Python environments on the HPC","text":"<p>The next step is to setup the Python environment. Effectively, when working on the cluster, this requires us to create a personal virtual environment.</p>"},{"location":"5_python_setup/#activating-and-managing-a-python-virtual-environment","title":"Activating and managing a Python Virtual Environment","text":"<p>The cluster has a huge set of modules, which are software packages that are available to all users. One such type of module is <code>python</code>, which itself has a large array of different software packages available through the cluster modules. To see which modules that are currently loaded, run the following command:</p> <pre><code>module list\n</code></pre> <p>To list all available modules, run:</p> <pre><code>module avail\n</code></pre> <p>To only show <code>python</code> modules, run:</p> <pre><code>module avail | grep ^python\n</code></pre> <p>Here, <code>grep</code> is a search command, that allows you to filter text to only show relevant output lines. Here, <code>python</code> is the search word. The <code>^</code> character in the above query ensures that a line must start with the search word.</p> <p>When running the command, you're prompted with several python versions which are available to you. Looking at the output list, you see there is a default choice, denoted by a string that ends with <code>(default)</code>. We can choose to load this module by simply running:</p> <pre><code>module load python3\n</code></pre> <p>If you'd like to run a specific python version, you have to specify the whole package in the above command, e.g. <code>module load python3/3.10.12</code>. </p> <p>Having loaded our desired python version, we're now ready to create a virtual environment. Run:</p> <pre><code>python3 -m venv &lt;path/to/venv&gt;\n</code></pre> <p><code>path/to/venv</code> indicates the relative- or absolute path to where you'd like to put your virtual environment, where <code>venv</code> is the name of the environment. We recommend putting it into your project directory root and adding it into your <code>.gitignore</code>-file.</p> <p>To finish up, we still need to activate the environment by running:</p> <pre><code>. /path/to/venv/bin/activate\n</code></pre> <p>You've now got python up and running!</p> <p>When you're finished using the environment, run the command:</p> <pre><code>deactivate\n</code></pre> <p>to deactivate the environment. \u00a0 Note: You'll need to load all modules and activate your environment each time you're in a new terminal session.</p>"},{"location":"5_python_setup/#installing-packages","title":"Installing packages","text":"<p>You may install a package in an activated virtual environment as follows:  <code>python -m pip install numpy</code></p> <p>the packages will then be installed in your virtual environment directory. </p>"},{"location":"5_python_setup/#a-complete-example","title":"A complete example","text":"<p>Examplewise, to create a python 3.10.12 environment called <code>dtu_hpc_intro</code> in your project-root called <code>hpc_folder</code>, activate it, install a pip package called <code>somepippackage</code> and followingly run a script, you would start out by issuing the following three commands in your terminal: </p> <pre><code>module load python3/3.10.12\ncd ~/hpc_folder \npython3 -m venv dtu_hpc_intro  \n</code></pre> <p>here, <code>~</code> indicates the user root of your directory and thus can be used no matter the current working directory. </p> <p>You can now activate the environment by writing one of the two following commands: </p> <pre><code>source ~/hpc_folder/dtu_hpc_intro/bin/activate\n. ~/hpc_folder/dtu_hpc_intro/bin/activate\n</code></pre> <p>in order to install your package, you now write: </p> <pre><code>python -m pip install somepippackage\n</code></pre> <p>and you can now run your script: </p> <pre><code>python3 src/my_python_file.py\n</code></pre>"},{"location":"5_python_setup/#streamlining-environment-activation","title":"Streamlining environment activation","text":"<p>Whenever you want to use your virtual environment, you need to specifically load the modules that you used to create the environment each time. This adds a bit of unneccessary overhead, needing to e.g. remember the specific modules you run each time.</p> <p>For this reason, we've added the simple script <code>setup_env.sh</code> into the GitHub repository, which streamlines the process for future uses. You can either copy this file or simply copy the contents, create a file with a similar name and paste it there. Importantly, you need to modify the file such that it fits your own needs. At a minimum, personalize the lines:</p> <pre><code># Change these lines\nmodule load python3\n# ...\n. /path/to/venv/bin/activate\n</code></pre> <p>You may also append other modules that you want to activate each time you enter the cluster. Lastly, you will need to update the permissions of the file, as it is currently not executable. Running the command:</p> <pre><code>chmod 700 path/to/setup_env.sh\n</code></pre> <p>will make the file executable for the file owner (you). To run the file, run:</p> <pre><code>source ./path/to/setup_env.sh\n</code></pre> <p>This one line will now do everything for you. The <code>source</code> prefix ensures, that your terminal remembers your actions post execution.</p>"},{"location":"5_python_setup/#a-note-on-collaboration","title":"A note on collaboration","text":"<p>If you collaborate with other students, it is best practice to agree on a specific version of python to use. Even better is it if you use the completely same package versions. This may effectively be handled using a requirements.txt file for pip. You can read more about this on the official pip documentation.</p>"},{"location":"5_python_setup/#new-commands-encountered-in-this-section","title":"New commands encountered in this section","text":"Command Function <code>module list</code> Lists all currently activated modules. <code>module avail</code> Lists all modules available on the cluster. <code>module load &lt;m&gt;</code> Loads module <code>m</code> on the cluster. <code>&lt;command1&gt; \\| &lt;command2&gt;</code> The pipe operator creates a pipeline between the standard output (stdout) of <code>command1</code> into the standard output of <code>command2</code>. <code>grep &lt;pattern&gt;</code> Searches input for lines that match a desired pattern and prints these lines. Often used with pipe operators."},{"location":"6_node_types/","title":"Running HPC Python code","text":"<p>In order to use the HPC properly, you need to know what nodes are, and how to use them. </p>"},{"location":"6_node_types/#node-types-and-their-uses","title":"Node types and their uses","text":"<p>The DTU cluster has four main types of nodes: login, application, interactive and compute nodes. A node is a single computer in the cluster which provides CPU, memory and potentially GPU for running tasks. Each node has a specific use-case, outlined below: </p> Type of node Function Login node Handles your connection into the cluster. You always ssh into a login-node, and this is where your VSCode editor session lives Application node Designed to run software and applications available at databars Interactive node Allows you to run code directly in a terminal or jupyter notebook Compute node Only allows running code using batch-jobs. Can either be CPU only or also have a GPU <p>Login-nodes: In all cases, please never run any programs on login nodes. They are to be used only for logging in, and running code will slow the nodes for all other HPC users. </p> <p>Interactive nodes are shared between all users. This means that when running code interactively, resources allocated to your program depends on how many other users use the specific interactive node. As a result, your code may run out of memory suddenly, or have slower/faster execution based on the general workload of the interactive node. For this reason, interactive nodes are best used for debugging, running jupyter notebooks and running light code, e.g. visualizations, but not well-suited for heavy tasks such as model training, inference, 3D shape analysis, etc. Given this specification, you're heavily encouraged to only debug actively, as you may be blocking other users from using GPU resources. There exist interactive nodes which only have a CPU, but some nodes in addition have a GPU.</p> <p>Compute nodes All heavy tasks should always be run on compute nodes.  In addition, if you're profiling code or report run-times of code in your work, the only proper way to measure it is to use compute nodes. The batch job you submit will allocate a specific amount of system resources for your program, and thus runtimes will not be influenced by general system use. In general, when you can use a compute node then you should, as this is the use which HPC is optimized for. Again, these nodes exist in CPU-only and GPU versions. Available GPU nodes (compute and interactive) are listed here. The next two subsections show how to use interactive and compute nodes.  </p> <p>Application nodes Are designed to run software and applications interactively, and are not relevant for this guide. </p>"},{"location":"6_node_types/#running-a-python-script-on-an-interactive-gpu-node","title":"Running a python script on an interactive GPU node","text":"<p>Assume you have a file file.py on the HPC cluster. Then you can run the file interactively by following these steps: </p> <ol> <li>ssh into the cluster</li> <li>change from a login session to an interactive session by entering a queue name into the terminal and pressing enter</li> <li>load your python virtual environment</li> <li>executing the code from the terminal by writing <code>python3 path/to/your/file.py</code></li> </ol> <p>Currently available interactive GPU queues are <code>sxm2sh</code>, <code>voltash</code> and <code>a100sh</code>. To start an interactive session, simply enter the node ID, e.g. <code>voltash</code>, in the terminal and press enter. You can log out from a node by running the <code>exit</code> command.</p> <p>In order to check the current workload of an interactive node, you can start a session on the node and enter <code>nvidia-smi</code>. This will provide terminal output similar to the below: </p> <pre><code>+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI (some version)  Driver Version: (some version)  CUDA Version: (some version) |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Titan X-PCIE-16GB              On  |  &lt;PCI Bus ID 0&gt;        |                    0 |\n| N/A   46C    P0             47W /  250W |    6468MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Titan X-PCIE-16GB              On  |  &lt;PCI Bus ID 1&gt;        |                    0 |\n| N/A   29C    P0             25W /  250W |       4MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A    &lt;process ID&gt;      C   ...some/path                           6152MiB |\n|    0   N/A  N/A    &lt;process ID&gt;      C   ...some/path                            310MiB |\n+-----------------------------------------------------------------------------------------+\n</code></pre> <p>Here, some information from the output has been redacted for security. </p> output interpretation <p>The key outputs of interest to you are mainly the GPU statistics. In this case above, we see the node has two GPUs, each of the type Titan X. Each is connected using PCIE, and has 16GB of memory.  The GPU with ID 0 is currently running at 47 watts with a temperature of 46 degrees celsius, and is currently using 6468MiB/16384MiB of memory, and thus potentially has resources for your job. Similarly, GPU 1 is unused.  In this case, if you were to run a script, you should run it on GPU 1, because else you may crash or slow down the large process running on GPU 0. If you already know memory requirements of your script, ensure that there actually is space for it on the GPU, as your job else will crash and you may crash others scripts as well.  Please always favor to run on unoccupied GPUs in order to not disturb other users.  </p> <p>In order to bind to a specific GPU in an interactive job, you will need to specify the CUDA device in pytorch, e.g.: </p> <p><code>a = torch.Tensor([1,2,3]).to(torch.device('cuda:1'))</code></p> <p>Our provided script run-file.sh sets CUDA_VISIBLE_DEVICES such that cuda:0 corresponds to the GPU with the most available free memory, and thus when using this you just need to do:</p> <p><code>a = torch.Tensor([1,2,3]).to(\"cuda\")</code></p> <p>In specific cases, the CUDA version may be important for you to download the correct version of pytorch.  </p>"},{"location":"6_node_types/#running-a-script-on-a-compute-node","title":"Running a script on a compute node","text":"<p>In order to run a script on a compute node, a batch-script is needed to tell the HPC system how many resources your program will take up. Basically, a batch-script can be understood as you placing an order on the HPC node. When placing the order, you create a batch-job which will be put in a queue. The scheduling system on the HPC then starts your script as soon as it is your turn in the queue, and the required resources are available. </p>"},{"location":"6_node_types/#the-batch-script","title":"The batch script","text":"<p>A batch script consists of a combination of batch-job code and bash code, and it should be saved with the file-extension <code>your_batch_filename.sh</code>. An example of a batch-script is provided below: </p> <pre><code>#!/bin/sh\n### The following section is the batch-job specific content which places your \"order\" \n### \u2013- specify queue where you want to run your job --\n#BSUB -q gpuv100\n### -- set the job name: this is used for monitoring your job later --\n#BSUB -J testjob\n### -- ask for number of CPU cores (in most cases just leave it as 4) --\n#BSUB -n 4\n### -- Select 1 gpu in exclusive process mode (in most cases leave it as is) --\n#BSUB -gpu \"num=1:mode=exclusive_process\"\n### -- set walltime limit: hh:mm --  maximum 24 hours for GPU-queues. Your job is killed if it exceeds.\n#BSUB -W 1:00\n### request 5GB of GPU-memory\n#BSUB -R \"rusage[mem=5GB]\"\n### set the job to run on a single host machine (don't change this)\n#BSUB -R \"span[hosts=1]\"\n### -- set an email address - a mail will be sent with some important statistics about your job and whether it succeeded/failed --\n#BSUB -u your_email_address\n### -- send e-mail notification when job starts -- \n#BSUB -B\n### -- send e-mail notification when job ends with statistics/status --\n#BSUB -N\n### -- Specify the file to which all your terminal output (e.g. print statements) are saved to\n### -- %J will here give it the JobID number which the scheduler assigns when submitting. \n#BSUB -o logs/gpu_%J.out\n### -- Specify the file to which all errors are saved to. Important for debugging if it crashes!\n#BSUB -e gpu_%J.err\n\n### -- end of batch job options - from here it is just shell code --\nmodule load python3/3.10.12\nsource path/to/your/environment\necho \"Training model...\"\nstart=$(date +%s) #example timer start\npython3 src/train_model.py  #train your model\nstop=$(date '+%s') #stop the timer to time how long it took\nelapsed=$(awk \"BEGIN {print $stop - $start}\")\necho \"training finished in $elapsed seconds\" #print how long it took\necho \"Running inference\"\npython3 src/model_inference.py\necho \"Plotting results...\"\npython3 src/some_result_plots.py\necho \"Finished all scripts. \"\n</code></pre> <p>The shell code is then executed as soon as it is your turn in the queue. </p> <p>Here, you may consider the shell code as simply being the same as interacting with the terminal line by line. As a result, you can stack a series of commands as done above which trains a model, followingly runs inference and plots some results while also timing how long model training takes and printing something to the output. </p> <p>Note that both <code>echo</code> (which is linux for print) and python script terminal outputs will be redirected to the <code>logs/gpu_%J.out</code> file instead of a terminal which you can see as it runs.</p>"},{"location":"6_node_types/#submitting-and-handling-a-batch-job","title":"Submitting and handling a batch job","text":"<p>In order to submit the job, enter the following in a terminal:  <code>bsub &lt; your_batch_filename.sh</code></p> <p>Your job will then receive a job ID. You can see whether your job is running using <code>bstat</code>.  You may always kill a job using <code>bkill &lt;jobID number&gt;</code>.</p> Debugging a batch-script by running as a shell-script <p>You may check that your batch script works by changing the permissions of the file to be executable by you:</p> <p><code>chmod 700 mybatchscript.sh</code></p> <p>And execute it in an interactive session using:</p> <p><code>./mybatchscript.sh</code></p> <p>which will run the shell-script contents but not submit to the queue when invoked in this way. When you're sure it works, press Ctrl and C to cancel execution, and then submit it to the queue using </p> <p><code>bsub &lt; mybatchscript.sh</code></p>"},{"location":"6_node_types/#checking-node-business","title":"Checking node business","text":"<p>You can check how busy different compute nodes are using the <code>bqueues</code> command. Here, the <code>PEND</code> column shows how many jobs are currently in queue. If you want to check a specific queue, in this case <code>gpua100</code>, you may write:  <code>bqueues | grep gpua100</code> to only get the corresponding line. Available compute nodes with GPUs are listed here. </p>"},{"location":"6_node_types/#new-commands-in-this-section","title":"New commands in this section","text":"<p>Reviewed commands:</p> Command Function <code>nvidia-smi</code> Lists interactive node information over the current active node. <code>bstat</code> Lists information of your current batch jobs. <code>bqueues</code> Lists activity of all compute nodes on cluster."},{"location":"7_debug/","title":"Remote debugging","text":"<p>As most Machine Learning / Deep Learning on the cluster is done using Python, a working debugging environment is incredibly valuable. </p> <p>Files will inevitably become linked over time as code complexity increases, making it increasingly cumbersome to keep track of logic flows after execution.  As debugging is not so straight forward when using the HPC, this section is dedicated to create a working debugging setup in a breeze for scripts running on interactive nodes.</p>"},{"location":"7_debug/#setting-up-the-debugging-environment","title":"Setting up the debugging environment","text":"<p>In order to get a working debugging setup you'll first need to install the Python Debugger extension by Microsoft in VSCode while connected to the HPC through Remote - SSH:</p> <p></p> <p>You'll also need the Python extension if you've not got this already.</p> <p>Now in the left column menu in your editor, click on the Run &amp; Debug section (the play button with an insect).</p> <p>This will either show an empty menu or you'll be prompted by some text and options, such as \"create a launch.json file\", and a large button with text \"Run and Debug\". </p> <p>If you see the latter case, then you don't have an existing launch.json file in your project root, which is necessary to start debugging. In this case:</p> <ol> <li>Click \"create a launch.json file\" </li> <li>In the dropdown, select \"Python Debugger\"</li> <li>Select \"Remote Attach\"</li> <li>Lastly, you'll need to specify a valid IP address and afterwards a valid port, for which the default options are fine for now; click enter twice. </li> </ol> <p>You'll now have a launch.json file in a hidden .vscode directory at your project root, which will look something like this:</p> <pre><code>{\n    go.microsoft.com/fwlink/?linkid=814321\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n\n        {\n            \"name\": \"Python Debugger: Remote Attach\",\n            \"type\": \"debugpy\",\n            \"request\": \"attach\",\n            \"connect\": {\n                \"host\": \"localhost\",\n                \"port\": 5678\n            },\n            \"pathMappings\": [\n                {\n                    \"localRoot\": \"${workspaceFolder}\",\n                    \"remoteRoot\": \".\"\n                }\n            ]\n        }\n    ]\n}\n</code></pre> If you can't find the file in the VSCode file editor <p>The VSCode file editor only shows a current snapshot of your project directory. Sometimes, new files/folders which you e.g. create with the terminal will only be visible if you press the \"Refresh explorer\" button in the top of the file explorer. </p> <p>The important thing to note here is everything within the \"configurations\" section of launch.json; here, all of your debugging setups have to be stored.  </p> <p>In your Run and Debug section on VS Code, there is now a dopdown menu at the top, which will hold each of your debug configurations which you can choose from as soon as we've created them.</p> <p></p> <p>Having created a launch.json file, we now only need to create a communication channel between your current interactive node and your terminal. We've created a script to do this for you, which appends a debugging configuration for your current interactive node session. Run the script add_debug_machine.sh to do so, by issuing these commands from the project root directory:</p> <pre><code>chmod 700 scripts/add_debug_machine.sh\n./scripts/add_debug_machine.sh\n</code></pre> <p>and add the name of the interactive node you're on. </p> <p>Note: you'll need to do this each time you're on a new interactive node which you haven't made a configuration for yet.</p> <p>Important: never share .vscode/launch.json online, as this exposes the open ports and ip adress of the interactive nodes. </p> <p>If you're in doubt, check that your <code>.gitignore</code> file in your project-root contains the line <code>.vscode/*</code> and that <code>git status</code> never includes the .vscode/ folder contents, even after running <code>git add .</code>.</p> <p>What the script actually does</p> <p>The script extracts a specific ethernet IP address and inserts it into the configurations section of launch.json. When starting a debugging session with the given configuration, the debugger waits and listens on a specific port of this ip-address to allow tunelling debugging. </p>"},{"location":"7_debug/#debugging-a-script","title":"Debugging a script","text":"<p>Now that our debugging environment is ready, we now want to use a test script to see if the debugger works as expected. Make a new python file and add some logic, such as:</p> <pre><code>print(\"hello\")\nx=12\nwhile x &lt; 1500:\n    x+=2\n    x+=1\nprint(x)\nx=6\n</code></pre> <p>Now we want to add a breakpoint, done by clicking to the left of the line indices in the python script. If you're unfamiliar with debugging, this means that when the debugger arrives at this point, it will come to an immediate stop, allowing you to inspect values of variables dynamically during execution. For the above code, try to set a breakpoint at line 5 - when debugging, the value of x should be 14.</p> <p>Now to start debugging, you need to run the run_file.sh script from the GitHub repository, using the command:</p> <pre><code>./scripts/run_file.sh path/to/test_file.py --debug\n</code></pre> <p>Upon executing the run_file.sh script, it will now wait until you attach your debugger onto your session. Just to the left of the dropdown debug configurations menu, there's a run button. Click this to attach your debugger (note: you need to wait for a couple of seconds, otherwise VS Code will complain). If everything works as expected, then execution starts and you should stop at your breakpoint, be able to print variables in an interactive terminal, etc.!</p> <p>One issue you may find is, that the debugger doesn't stop at your breakpoint. If this is the case, the likely reason is that your editor (VSCode) is not stationed at your project root, which is necessary for your debugger to work. Change directory and remember to move your .vscode directory to your project root.</p>"},{"location":"7_debug/#the-run-filesh-helper-script-for-runningdebugging-scripts","title":"The run-file.sh helper script for running/debugging scripts","text":"<p>Elaborating on the run_file.sh file, its intended purpose is to streamline cluster interaction, such that all you need to do in order to run/debug (mostly) successfully:</p> <ol> <li>ssh into the server at the project root</li> <li>Connect to the given interactive node of your choice</li> <li>Run run_file.sh script on a desired python file</li> </ol> <p>The script will itself choose the GPU which has most available memory, meaning that you don't need to worry about commands such as nvidia-smi. If you can't run the script, then this subsequently means, that there is no space available on the current node!</p> <p>Furthermore, the script itself calls setup_env.sh, meaning you don't have to think about your python environment. Finally, it handles the debugging configuration for you, as per the previous section.</p> <p>There are two variables you may need to change, depending on your application. These can be found in the \"personalized variables\" file section:</p> <pre><code># ---- Personalized variables ---- #\n\nFILEPATH=\"src/main.py\"\nPORT=\"5678\"\n\n# ---- Personalized variables ---- #\n</code></pre> <p>Changing the FILEPATH variable will set the default file to be executed by the script, i.e. the script invoked when simply issuing  <code>./scripts/run_file.sh</code></p> <p>You therefore don't need to specify this path in the command line, but can always overwrite it by providing a path when invoking the script.</p> <p>The PORT variable may never be needed, however, if you can't attach to the debugger, this may be the reason. If so, change this port number and match the port numbers found in launch.json with your chosen value.</p>"},{"location":"9_file_transfer/","title":"Transferring files to and from the cluster","text":"<p>You have two options of sending data to the DTU HPC system; either via. a transfer from your computer, or via. a direct download.</p> <p>Important: Please ensure that you only copy data from trusted sources to the DTU server, i.e. datasets provided by your project supervisor or original download links which are associated with peer-reviewed papers from trusted journals, as you may else infect the DTU HPC. </p>"},{"location":"9_file_transfer/#transfer-from-your-computer","title":"Transfer from your computer","text":"<p>This is the preferred method. Never transfer data using login, interactive or compute nodes; the HPC has a transfer server, which is designed for high speed transfers.</p>"},{"location":"9_file_transfer/#local-remote","title":"local \u2192 remote","text":"<p>This can be done using the following command on a local terminal:</p> <pre><code>scp -r path/to/local/data &lt;study-number&gt;@transfer.gbar.dtu.dk:/path/to/your/project\n</code></pre> <p>Here, scp (secure copy) copies the file contents from your machine onto the remote location you've specified. The tag -r allows a recursive copy, meaning a directory may be copied in its entirety onto the cluster. Importantly, you need to specify the absolute path to the project on the server.</p>"},{"location":"9_file_transfer/#remote-local","title":"remote \u2192 local","text":"<p>This can either be done in a similar way by using a local terminal the opposite way round:</p> <pre><code>scp -r &lt;study-number&gt;@transfer.gbar.dtu.dk:/path/to/your/project/data path/to/local/destination\n</code></pre> <p>For small files, you may also employ a simpler method. In VS Code, right click the file you want to retrieve from the remote. Here, you'll see a download option - this will download the file onto your local machine. This, however, should never be done for larger files, as this slows down login nodes for all users.</p>"},{"location":"9_file_transfer/#direct-download","title":"Direct download","text":"<p>Please only use this method when you are absolutely sure that you know you are downloading the correct archived file, and only use this method when you have no other choice.  </p>"},{"location":"9_file_transfer/#kaggle-remote","title":"kaggle \u2192 remote","text":"<p>If you want to download a dataset from kaggle, e.g. with the URL https://www.kaggle.com/datasets/welovehpc/satellite-images, you may <code>cd</code> into your project root, <code>cd</code> into a data folder, and then use the command </p> <pre><code>curl -L -o data.zip\n  https://www.kaggle.com/api/v1/datasets/download/welovehpc/satellite-images\n</code></pre> <p>This will allow you to download directly on the HPC server. Please only do this on interactive nodes.</p>"},{"location":"9_file_transfer/#some-filesharing-service-remote","title":"Some filesharing service \u2192 remote","text":"<p>The <code>curl</code> approach is also useful if you have a download link for an arbitrary filesharing service, and you do not have space to download the dataset onto your own machine; here you would simply curl the download link. This usually works for filesharing services where you are able to get the URL of the file, usually by right-clicking the download button and copying the URL. This, of course, only works for archived files, e.g. zip, and similar:</p> <pre><code>curl wetransfer.com/some_dataset.zip\n</code></pre> <p>This works on google drive and some other arbitrary file sharing services.</p>"},{"location":"9_file_transfer/#storage","title":"Storage","text":"<p>Each student has a default storage limit of 30GB on the HPC. For bachelor's or master's projects where higher capacity is necessary, write a request for more storage to the HPC staff in due time.</p>"},{"location":"9_file_transfer/#a-complete-step-by-step-example-for-file-transfer-from-a-local-machine","title":"A complete step-by-step example for file transfer from a local machine","text":"<p>Assume you have a <code>zip</code>-file located in the path /home/max/Documents/DTU/my_cool_dataset.zip, which will unzip into the structure: </p> <pre><code>my_cool_dataset.zip\n\u2514\u2500\u2500 data\n    \u251c\u2500\u2500 file_1_image.npy\n    \u251c\u2500\u2500 file_1_label.npy\n    \u251c\u2500\u2500 file_2_image.npy\n    \u251c\u2500\u2500 file_2_label.npy\n    \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 splits\n    \u251c\u2500\u2500 train_idx.txt\n    \u251c\u2500\u2500 val_idx.txt\n    \u251c\u2500\u2500 test_idx.txt    \n</code></pre> <p>You want to copy this file to the HPC into your project root directory which is called mycoolproject, and your student number is <code>s123456</code>. </p> <p>Your goal is to achieve the following file-structure for your project: </p> <pre><code>mycoolproject\n\u2514\u2500\u2500 data\n    \u2514\u2500\u2500 raw_images\n        \u251c\u2500\u2500 1.npy\n        \u251c\u2500\u2500 ...\n    \u2514\u2500\u2500 raw_labels\n        \u251c\u2500\u2500 1.npy\n        \u251c\u2500\u2500 ... \n    \u2514\u2500\u2500 processed_images\n        \u251c\u2500\u2500 1.npy\n        \u251c\u2500\u2500 ...\n    \u2514\u2500\u2500 processed_labels\n        \u251c\u2500\u2500 1.txt\n        \u251c\u2500\u2500 ... \n    \u2514\u2500\u2500 splits\n        \u251c\u2500\u2500 train.txt\n        \u251c\u2500\u2500 val.txt\n        \u251c\u2500\u2500 test.txt    \n\u2514\u2500\u2500 logs\n    \u251c\u2500\u2500 log-1.txt\n    \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 outputs\n    \u251c\u2500\u2500 out.txt\n    \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 scripts\n    \u251c\u2500\u2500 add_debug_machine.sh\n    \u251c\u2500\u2500 run_file.sh\n    \u251c\u2500\u2500 setup_env.sh\n    \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 main.py\n    \u251c\u2500\u2500 preprocess.py\n    \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 README.md\n</code></pre> <p>so you start out by logging into the HPC from a terminal: </p> <pre><code>ssh s123456@login1.gbar.dtu.dk\n</code></pre> <p>then change your working directory to your project root: </p> <pre><code>cd mycoolproject\n</code></pre> <p>check whether you already have a data folder: </p> <pre><code>ls\n</code></pre> <p>if no data-folder is present, you make one: </p> <pre><code>mkdir data\n</code></pre> <p>and <code>cd</code> into it: </p> <pre><code>cd data\n</code></pre> <p>and copy the terminal heading which should say something along the lines of: </p> <pre><code>~/mycoolproject/data\n</code></pre> <p>Copy this. We need it for the <code>scp</code> transfer target. </p> Blackhole folders <p>This is only relevant if you have requested more storage from HPC staff. In this case, it will often be located in a blackhole folder, which is not directly accessible using the tilde-operator, as it lies on a different file-system. In this case, you will have to cd into the blackhole path, make the directories you need, and finally copy the complete path. You can print the complete path using the <code>pwd</code>-command. </p> <p>Now, on your local machine, open up a terminal. Now, we will copy the <code>.zip</code>-file by writing: </p> <pre><code>scp -i ~/.ssh/gbar /home/max/Documents/DTU/my_cool_dataset.zip s123456@transfer.gbar.dtu.dk:~/mycoolproject/data/\n</code></pre> <p>and verify using our credentials. Now <code>ls</code> on the ssh-connection. You should see the folder now contains <code>my_cool_dataset.zip</code>-file being present in your directory. </p> <p>Now, we will unzip it using: </p> <pre><code>unzip my_cool_dataset.zip\n</code></pre> <p>which will create a folder called my_cool_dataset. First, let's handle the <code>splits</code> folder. We move it up one to the current path depth using <code>mv</code>: </p> <pre><code>mv my_cool_dataset/splits .\n</code></pre> <p>which means \"move the complete folder my_cool_dataset/splits to where I currently am\". We will need to rename all the files to fit our directory structure goal. thiss can be done using <code>mv</code>:</p> <pre><code>mv splits/train_idx.txt splits/train.txt\nmv splits/val_idx.txt splits/val.txt\nmv splits/test_idx.txt splits/test.txt\n</code></pre> <p>Next, make the raw-directories for the labels and the images in your data-directory using <code>mkdir</code>. </p> <p>We can also use the <code>mv</code> command using wild-card expressions. Let's do so to seperate images from labels: </p> <pre><code>mv my_cool_dataset/data/*_image.npy raw_images/*_image.npy\n</code></pre> <p>and the labels: </p> <pre><code>mv my_cool_dataset/data/*_label.npy raw_labels/*_label.npy\n</code></pre> Renaming many files using <code>rename</code> and a shell wildcard <p>You can rename many files in a directory using the <code>rename</code> command. On the HPC, the syntax is:</p> <p><code>rename 'string_to_match' 'replacement_string' target_files</code></p> <p>For example, to strip filename clutter from image files:</p> <p><code>rename 'file_' '' *.npy rename '_image' '' *.npy</code></p> <p>This performs a literal string replacement on each filename.</p> <p>Adding the <code>-v</code> flag makes <code>rename</code> verbose and prints each rename operation.</p> <p>Now we've extracted all contents of the dataset, and can remove it safely using: </p> <pre><code>rm -r my_cool_dataset\n</code></pre> <p>and </p> <pre><code>rm my_cool_dataset.zip\n</code></pre> Tip <p>Unless your dataset is very large, we recommend keeping the .zip of the dataset saved somewhere on the HPC, such that you do not have to re-transfer the dataset if you accidentally delete it. </p>"}]}